{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGGnetCNN with Mnist Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_ZBrQv-8gvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsRz6ilTwXnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation, Reshape\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljbrwEbM-77m",
        "colab_type": "code",
        "outputId": "46bbca12-3ab4-4dde-d605-e013a59d3a8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(\"Train Set Size = {} images\".format(y_train.shape[0]))\n",
        "print(\"Test Set Size = {} images\".format(y_test.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "Train Set Size = 60000 images\n",
            "Test Set Size = 10000 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pId_NjjWNMUQ",
        "colab_type": "code",
        "outputId": "8f611856-1bde-4a60-c9d5-f2354bfc0e39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "fig1 = plt.figure(figsize = (15,15))\n",
        "\n",
        "for i in range(5):\n",
        "    ax1 = fig1.add_subplot(1,5,i+1) \n",
        "    ax1.imshow(x_train[i], interpolation='none', cmap=plt.cm.gray)\n",
        "    ax2 = fig1.add_subplot(2,5,i+6) \n",
        "    ax2.imshow(x_train[i+6], interpolation='none', cmap=plt.cm.gray)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2cAAAGPCAYAAADYw6ijAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nid4xLh44CO8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x1080 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzYASZ3-MEMS",
        "colab_type": "code",
        "outputId": "5528d9ed-abf6-47cc-ea5f-65d933746f99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(\"Labels : {}\".format(y_train[0:5]))\n",
        "print(\"Labels : {}\".format(y_train[6:11]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels : [5 0 4 1 9]\n",
            "Labels : [1 3 1 4 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kZpTbw1O1lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_onehot = to_categorical(y_train, num_classes=None, dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evF929xYwnYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = RMSprop(lr=1e-4)\n",
        "objective = 'binary_crossentropy'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqrjOid8YRRw",
        "colab_type": "code",
        "outputId": "8eaed163-f0a7-4e56-8a3d-fc6a2fb531cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "def vgg16():\n",
        "    model = Sequential()\n",
        "    model.add(Reshape((28, 28, 1)))\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', input_shape=((28, 28, 1)), activation='relu'))\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(data_format=\"channels_last\", pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(data_format=\"channels_last\", pool_size=(2, 2)))\n",
        "    \n",
        "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(data_format=\"channels_last\", pool_size=(2, 2)))\n",
        "    \n",
        "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(data_format=\"channels_last\", pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    #model.add(MaxPooling2D(data_format=\"channels_last\", pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss=objective, optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = vgg16()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y179ha5rwhFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UYKOcvee-Uy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_epoch = 50\n",
        "batch_size = 128\n",
        "\n",
        "## Callback for loss logging per epoch\n",
        "class LossHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.acc = []\n",
        "        self.val_acc = []\n",
        "        \n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.acc.append(logs.get('acc'))\n",
        "        self.val_acc.append(logs.get('val_acc'))\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=100, verbose=1, mode='auto')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML-Mv6hFwtlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_vgg16():\n",
        "    \n",
        "    history = LossHistory()\n",
        "    model.fit(x_train, y_train_onehot, batch_size=batch_size, epochs=nb_epoch,\n",
        "              validation_split=0.25, verbose=1, shuffle=True, callbacks=[history, early_stopping])\n",
        "    \n",
        "\n",
        "    predictions = model.predict(x_test, verbose=0)\n",
        "    return predictions, history\n",
        "\n",
        "def test_accuracy():\n",
        "    err = []\n",
        "    t = 0\n",
        "    for i in range(predictions.shape[0]):\n",
        "        if (np.argmax(predictions[i]) == y_test[i]):\n",
        "            t = t+1\n",
        "        else:\n",
        "            err.append(i)\n",
        "    return t, float(t)*100/predictions.shape[0], err"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quna8bB2xjQ-",
        "colab_type": "code",
        "outputId": "bcd97e1e-8d1c-4aa9-a204-091725ce8de0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "predictions, history = run_vgg16()\n",
        "q = test_loss()\n",
        "p = test_accuracy()\n",
        "print(\"Test accuracy: {} %\".format(p[1]))\n",
        "print(\"Test Loss: {} %\".format(1[0]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 45000 samples, validate on 15000 samples\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "45000/45000 [==============================] - 29s 648us/step - loss: 0.1169 - acc: 0.9614 - val_loss: 0.0220 - val_acc: 0.9941\n",
            "Epoch 2/50\n",
            "45000/45000 [==============================] - 14s 309us/step - loss: 0.0228 - acc: 0.9940 - val_loss: 0.0263 - val_acc: 0.9941\n",
            "Epoch 3/50\n",
            "45000/45000 [==============================] - 14s 308us/step - loss: 0.0134 - acc: 0.9966 - val_loss: 0.0120 - val_acc: 0.9974\n",
            "Epoch 4/50\n",
            "45000/45000 [==============================] - 14s 308us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0096 - val_acc: 0.9978\n",
            "Epoch 5/50\n",
            "45000/45000 [==============================] - 14s 307us/step - loss: 0.0077 - acc: 0.9981 - val_loss: 0.0100 - val_acc: 0.9977\n",
            "Epoch 6/50\n",
            "45000/45000 [==============================] - 14s 307us/step - loss: 0.0060 - acc: 0.9984 - val_loss: 0.0139 - val_acc: 0.9969\n",
            "Epoch 7/50\n",
            "45000/45000 [==============================] - 14s 308us/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.0108 - val_acc: 0.9981\n",
            "Epoch 8/50\n",
            "45000/45000 [==============================] - 14s 308us/step - loss: 0.0044 - acc: 0.9990 - val_loss: 0.0134 - val_acc: 0.9974\n",
            "Epoch 9/50\n",
            "45000/45000 [==============================] - 14s 307us/step - loss: 0.0044 - acc: 0.9990 - val_loss: 0.0081 - val_acc: 0.9983\n",
            "Epoch 10/50\n",
            "45000/45000 [==============================] - 14s 307us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.0223 - val_acc: 0.9975\n",
            "Epoch 11/50\n",
            "45000/45000 [==============================] - 14s 307us/step - loss: 0.0036 - acc: 0.9991 - val_loss: 0.0089 - val_acc: 0.9984\n",
            "Epoch 12/50\n",
            "45000/45000 [==============================] - 14s 302us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 0.0108 - val_acc: 0.9982\n",
            "Epoch 13/50\n",
            "45000/45000 [==============================] - 14s 300us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 0.0190 - val_acc: 0.9976\n",
            "Epoch 14/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 0.0155 - val_acc: 0.9982\n",
            "Epoch 15/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0036 - acc: 0.9994 - val_loss: 0.0151 - val_acc: 0.9980\n",
            "Epoch 16/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0038 - acc: 0.9993 - val_loss: 0.0178 - val_acc: 0.9983\n",
            "Epoch 17/50\n",
            "45000/45000 [==============================] - 14s 300us/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.0106 - val_acc: 0.9975\n",
            "Epoch 18/50\n",
            "45000/45000 [==============================] - 14s 302us/step - loss: 0.0033 - acc: 0.9994 - val_loss: 0.0121 - val_acc: 0.9984\n",
            "Epoch 19/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0033 - acc: 0.9995 - val_loss: 0.0138 - val_acc: 0.9984\n",
            "Epoch 20/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0045 - acc: 0.9993 - val_loss: 0.0111 - val_acc: 0.9974\n",
            "Epoch 21/50\n",
            "45000/45000 [==============================] - 14s 302us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 0.0103 - val_acc: 0.9983\n",
            "Epoch 22/50\n",
            "45000/45000 [==============================] - 13s 299us/step - loss: 0.0051 - acc: 0.9993 - val_loss: 0.0138 - val_acc: 0.9983\n",
            "Epoch 23/50\n",
            "45000/45000 [==============================] - 14s 300us/step - loss: 0.0043 - acc: 0.9993 - val_loss: 0.0149 - val_acc: 0.9983\n",
            "Epoch 24/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0037 - acc: 0.9995 - val_loss: 0.0153 - val_acc: 0.9977\n",
            "Epoch 25/50\n",
            "45000/45000 [==============================] - 13s 299us/step - loss: 0.0044 - acc: 0.9994 - val_loss: 0.0155 - val_acc: 0.9983\n",
            "Epoch 26/50\n",
            "45000/45000 [==============================] - 14s 305us/step - loss: 0.0042 - acc: 0.9994 - val_loss: 0.0247 - val_acc: 0.9980\n",
            "Epoch 27/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0053 - acc: 0.9992 - val_loss: 0.0176 - val_acc: 0.9983\n",
            "Epoch 28/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0071 - acc: 0.9991 - val_loss: 0.0176 - val_acc: 0.9983\n",
            "Epoch 29/50\n",
            "45000/45000 [==============================] - 14s 304us/step - loss: 0.0085 - acc: 0.9990 - val_loss: 0.0169 - val_acc: 0.9981\n",
            "Epoch 30/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0166 - acc: 0.9986 - val_loss: 0.0281 - val_acc: 0.9978\n",
            "Epoch 31/50\n",
            "45000/45000 [==============================] - 14s 300us/step - loss: 0.0073 - acc: 0.9993 - val_loss: 0.0328 - val_acc: 0.9975\n",
            "Epoch 32/50\n",
            "45000/45000 [==============================] - 13s 299us/step - loss: 0.0061 - acc: 0.9994 - val_loss: 0.0205 - val_acc: 0.9983\n",
            "Epoch 33/50\n",
            "45000/45000 [==============================] - 13s 300us/step - loss: 0.0065 - acc: 0.9993 - val_loss: 0.0254 - val_acc: 0.9981\n",
            "Epoch 34/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0069 - acc: 0.9993 - val_loss: 0.0395 - val_acc: 0.9974\n",
            "Epoch 35/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0093 - acc: 0.9992 - val_loss: 0.0248 - val_acc: 0.9981\n",
            "Epoch 36/50\n",
            "45000/45000 [==============================] - 14s 302us/step - loss: 0.0063 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9978\n",
            "Epoch 37/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0067 - acc: 0.9993 - val_loss: 0.0209 - val_acc: 0.9982\n",
            "Epoch 38/50\n",
            "45000/45000 [==============================] - 14s 300us/step - loss: 0.0061 - acc: 0.9994 - val_loss: 0.0384 - val_acc: 0.9974\n",
            "Epoch 39/50\n",
            "45000/45000 [==============================] - 14s 304us/step - loss: 0.0196 - acc: 0.9986 - val_loss: 0.0164 - val_acc: 0.9977\n",
            "Epoch 40/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0075 - acc: 0.9992 - val_loss: 0.0191 - val_acc: 0.9982\n",
            "Epoch 41/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0083 - acc: 0.9992 - val_loss: 0.0200 - val_acc: 0.9981\n",
            "Epoch 42/50\n",
            "45000/45000 [==============================] - 13s 300us/step - loss: 0.0081 - acc: 0.9992 - val_loss: 0.0165 - val_acc: 0.9980\n",
            "Epoch 43/50\n",
            "45000/45000 [==============================] - 13s 299us/step - loss: 0.0063 - acc: 0.9993 - val_loss: 0.0220 - val_acc: 0.9981\n",
            "Epoch 44/50\n",
            "45000/45000 [==============================] - 14s 300us/step - loss: 0.0149 - acc: 0.9988 - val_loss: 0.0201 - val_acc: 0.9973\n",
            "Epoch 45/50\n",
            "45000/45000 [==============================] - 14s 300us/step - loss: 0.0059 - acc: 0.9993 - val_loss: 0.0232 - val_acc: 0.9974\n",
            "Epoch 46/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0105 - acc: 0.9992 - val_loss: 0.0303 - val_acc: 0.9979\n",
            "Epoch 47/50\n",
            "45000/45000 [==============================] - 14s 301us/step - loss: 0.0084 - acc: 0.9991 - val_loss: 0.0225 - val_acc: 0.9983\n",
            "Epoch 48/50\n",
            "45000/45000 [==============================] - 14s 304us/step - loss: 0.0047 - acc: 0.9995 - val_loss: 0.0225 - val_acc: 0.9983\n",
            "Epoch 49/50\n",
            "45000/45000 [==============================] - 14s 307us/step - loss: 0.0064 - acc: 0.9994 - val_loss: 0.0227 - val_acc: 0.9981\n",
            "Epoch 50/50\n",
            "45000/45000 [==============================] - 14s 304us/step - loss: 0.0117 - acc: 0.9990 - val_loss: 0.0354 - val_acc: 0.9974\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d1d029023107>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predictions, history = run_vgg16()\\nq = test_loss()\\np = test_accuracy()\\nprint(\"Test accuracy: {} %\".format(p[1]))\\nprint(\"Test Loss: {} %\".format(1[0]))'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_loss' is not defined"
          ]
        }
      ]
    }
  ]
}